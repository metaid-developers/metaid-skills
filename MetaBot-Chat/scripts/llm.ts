#!/usr/bin/env node

/**
 * LLM Integration Module
 * Supports Deepseek, OpenAI, Claude, Gemini for generating intelligent, context-aware responses.
 * é…ç½®è§£æä¼˜å…ˆçº§ï¼šaccount.json çš„ accountList[].llm > config.json/ .env é»˜è®¤é…ç½®ã€‚
 */

import { getEnv } from './env-config'

export interface LLMConfig {
  provider: 'openai' | 'claude' | 'deepseek' | 'gemini' | 'custom'
  apiKey?: string
  baseUrl?: string
  model?: string
  temperature?: number
  maxTokens?: number
}

export interface LLMMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export interface LLMResponse {
  content: string
  usage?: {
    promptTokens?: number
    completionTokens?: number
    totalTokens?: number
  }
}

// é»˜è®¤é…ç½®ï¼ˆä¸å« apiKeyï¼ŒapiKey å¿…é¡»ä» .envã€config æˆ– account ä¼ å…¥ï¼‰
const DEFAULT_CONFIG: Partial<LLMConfig> = {
  provider: 'deepseek',
  model: 'DeepSeek-V3.2',
  baseUrl: 'https://api.deepseek.com',
  temperature: 0.8,
  maxTokens: 6000,
}

/** æŒ‰ provider ä» env å– API Keyï¼ˆä¸ env-config çš„ configFromEnv ä¸€è‡´ï¼‰ */
function getApiKeyFromEnv(provider: string, env?: Record<string, string>): string {
  const e = env ?? getEnv()
  return (
    e.LLM_API_KEY ||
    e.DEEPSEEK_API_KEY ||
    e.OPENAI_API_KEY ||
    e.CLAUDE_API_KEY ||
    e.GEMINI_API_KEY ||
    ''
  )
}

/** æŒ‰ provider å–é»˜è®¤æ¨¡å‹å */
function defaultModel(provider: string): string {
  switch (provider) {
    case 'gemini':
      return 'gemini-2.0-flash'
    case 'openai':
      return 'gpt-4o-mini'
    case 'claude':
      return 'claude-3-5-sonnet-20241022'
    default:
      return 'deepseek-chat'
  }
}

/** æ ‡å‡†åŒ–æ¨¡å‹åï¼ˆç”¨äºå…¼å®¹ config ä¸­å†™çš„ DeepSeek-V3.2 ç­‰ï¼‰ */
function normalizeModel(provider: string, model?: string): string {
  if (!model) return defaultModel(provider)
  if (provider === 'deepseek' && (model === 'DeepSeek-V3.2' || model === 'DeepSeek-V3')) return 'deepseek-chat'
  return model
}

export type ResolvedLLMConfig = Partial<LLMConfig>

/**
 * è§£ææœ€ç»ˆä½¿ç”¨çš„ LLM é…ç½®ï¼ˆä¾› generateLLMResponse ç­‰ä½¿ç”¨ï¼‰
 * ä¼˜å…ˆçº§ï¼šaccount.json çš„ accountList[].llmï¼ˆä¸”å« apiKeyï¼‰> config.json / .env é»˜è®¤é…ç½®
 * @param account å½“å‰è´¦æˆ·ï¼ˆå¦‚ findAccountByUsername çš„è¿”å›å€¼ï¼‰ï¼Œè‹¥æœ‰ llm ä¸”å¸¦ apiKey åˆ™ä¼˜å…ˆä½¿ç”¨
 * @param config å…¨å±€ configï¼ˆå¦‚ readConfig()ï¼‰ï¼Œå…¶ llm å·²ç”± normalizeConfig åˆå¹¶è¿‡ .env çš„ apiKey
 */
export function getResolvedLLMConfig(
  account?: { llm?: unknown } | null,
  config?: { llm?: Partial<LLMConfig> }
): ResolvedLLMConfig {
  const accountLlmRaw = account?.llm
  const accountLlm =
    accountLlmRaw != null
      ? (Array.isArray(accountLlmRaw) ? (accountLlmRaw as Partial<LLMConfig>[])[0] : (accountLlmRaw as Partial<LLMConfig>))
      : undefined
  const hasAccountLlm = accountLlm?.apiKey != null && String(accountLlm.apiKey).trim() !== ''
  const base = config?.llm ?? {}

  const provider = (hasAccountLlm ? accountLlm!.provider : base.provider) || 'deepseek'
  const prov = provider as LLMConfig['provider']
  const apiKey = hasAccountLlm
    ? accountLlm!.apiKey
    : (base.apiKey || getApiKeyFromEnv(provider))
  const model = hasAccountLlm
    ? normalizeModel(provider, accountLlm!.model)
    : normalizeModel(provider, base.model) || defaultModel(provider)
  const baseUrl =
    (hasAccountLlm ? accountLlm!.baseUrl : base.baseUrl) ||
    (provider === 'gemini'
      ? 'https://generativelanguage.googleapis.com'
      : provider === 'deepseek'
        ? 'https://api.deepseek.com'
        : provider === 'openai'
          ? 'https://api.openai.com/v1'
          : provider === 'claude'
            ? 'https://api.anthropic.com/v1'
            : undefined)

  return {
    provider: prov,
    apiKey,
    baseUrl,
    model,
    temperature: hasAccountLlm ? accountLlm!.temperature : base.temperature,
    maxTokens: hasAccountLlm ? accountLlm!.maxTokens : base.maxTokens,
  }
}

/**
 * Generate response using LLM
 */
export async function generateLLMResponse(
  messages: LLMMessage[],
  config?: Partial<LLMConfig>
): Promise<LLMResponse> {
  const finalConfig = { ...DEFAULT_CONFIG, ...config } as LLMConfig

  // Check if API key is provided
  if (!finalConfig.apiKey) {
    throw new Error(
      'LLM API key not configured. Please set DEEPSEEK_API_KEY, OPENAI_API_KEY, CLAUDE_API_KEY or GEMINI_API_KEY in .env, or configure in account.json / config.json'
    )
  }

  switch (finalConfig.provider) {
    case 'deepseek':
      return await callDeepseek(messages, finalConfig)
    case 'openai':
      return await callOpenAI(messages, finalConfig)
    case 'claude':
      return await callClaude(messages, finalConfig)
    case 'gemini':
      return await callGemini(messages, finalConfig)
    default:
      throw new Error(`Unsupported LLM provider: ${finalConfig.provider}`)
  }
}

/**
 * Call Deepseek API (OpenAI compatible)
 */
async function callDeepseek(
  messages: LLMMessage[],
  config: LLMConfig
): Promise<LLMResponse> {
  const baseUrl = config.baseUrl || 'https://api.deepseek.com'
  const model = config.model || 'DeepSeek-V3.2'
  // Deepseek API endpoint: https://api.deepseek.com/v1/chat/completions
  // Ensure baseUrl ends with /v1 for chat/completions endpoint
  let apiBaseUrl = baseUrl
  if (!apiBaseUrl.endsWith('/v1')) {
    apiBaseUrl = apiBaseUrl.endsWith('/') ? `${apiBaseUrl}v1` : `${apiBaseUrl}/v1`
  }

  const response = await fetch(`${apiBaseUrl}/chat/completions`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${config.apiKey}`,
    },
    body: JSON.stringify({
      model,
      messages: messages.map((msg) => ({
        role: msg.role,
        content: msg.content,
      })),
      temperature: config.temperature || 0.8,
      max_tokens: config.maxTokens || 500,
    }),
  })

  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`Deepseek API error: ${response.status} ${errorText}`)
  }

  const data = await response.json()
  return {
    content: data.choices[0]?.message?.content || '',
    usage: data.usage
      ? {
          promptTokens: data.usage.prompt_tokens,
          completionTokens: data.usage.completion_tokens,
          totalTokens: data.usage.total_tokens,
        }
      : undefined,
  }
}

/**
 * Call OpenAI API
 */
async function callOpenAI(
  messages: LLMMessage[],
  config: LLMConfig
): Promise<LLMResponse> {
  const baseUrl = config.baseUrl || 'https://api.openai.com/v1'
  const model = config.model || 'gpt-4o-mini'

  const response = await fetch(`${baseUrl}/chat/completions`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${config.apiKey}`,
    },
    body: JSON.stringify({
      model,
      messages: messages.map((msg) => ({
        role: msg.role,
        content: msg.content,
      })),
      temperature: config.temperature || 0.8,
      max_tokens: config.maxTokens || 500,
    }),
  })

  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`OpenAI API error: ${response.status} ${errorText}`)
  }

  const data = await response.json()
  return {
    content: data.choices[0]?.message?.content || '',
    usage: data.usage
      ? {
          promptTokens: data.usage.prompt_tokens,
          completionTokens: data.usage.completion_tokens,
          totalTokens: data.usage.total_tokens,
        }
      : undefined,
  }
}

/**
 * Call Claude API (Anthropic)
 */
async function callClaude(
  messages: LLMMessage[],
  config: LLMConfig
): Promise<LLMResponse> {
  const baseUrl = config.baseUrl || 'https://api.anthropic.com/v1'
  const model = config.model || 'claude-3-5-sonnet-20241022'

  // Convert messages to Claude format
  // Claude requires system message to be separate
  const systemMessage = messages.find((m) => m.role === 'system')?.content || ''
  const conversationMessages = messages
    .filter((m) => m.role !== 'system')
    .map((m) => ({
      role: m.role === 'assistant' ? 'assistant' : 'user',
      content: m.content,
    }))

  const response = await fetch(`${baseUrl}/messages`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': config.apiKey!,
      'anthropic-version': '2023-06-01',
    },
    body: JSON.stringify({
      model,
      max_tokens: config.maxTokens || 500,
      temperature: config.temperature || 0.8,
      system: systemMessage || undefined,
      messages: conversationMessages,
    }),
  })

  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`Claude API error: ${response.status} ${errorText}`)
  }

  const data = await response.json()
  return {
    content: data.content[0]?.text || '',
    usage: data.usage
      ? {
          promptTokens: data.usage.input_tokens,
          completionTokens: data.usage.output_tokens,
          totalTokens: data.usage.input_tokens + data.usage.output_tokens,
        }
      : undefined,
  }
}

/**
 * Call Google Gemini API (Generative Language API)
 * é»˜è®¤æ¨¡å‹ï¼šgemini-2.0-flashï¼ˆå¯¹åº”ã€ŒGemini 3 Flashã€ç­‰å‘½åï¼Œå¯ç» LLM_MODEL è¦†ç›–ï¼‰
 */
async function callGemini(
  messages: LLMMessage[],
  config: LLMConfig
): Promise<LLMResponse> {
  const baseUrl = config.baseUrl || 'https://generativelanguage.googleapis.com'
  const model = config.model || 'gemini-2.0-flash'
  const apiKey = config.apiKey!

  const systemMessage = messages.find((m) => m.role === 'system')?.content || ''
  const conversationMessages = messages.filter((m) => m.role !== 'system')

  const contents: { role: string; parts: { text: string }[] }[] = []
  for (const msg of conversationMessages) {
    const role = msg.role === 'assistant' ? 'model' : 'user'
    contents.push({ role, parts: [{ text: msg.content }] })
  }

  const body: Record<string, unknown> = {
    contents,
    generationConfig: {
      temperature: config.temperature ?? 0.8,
      maxOutputTokens: config.maxTokens ?? 500,
    },
  }
  if (systemMessage) {
    body.systemInstruction = { parts: [{ text: systemMessage }] }
  }

  const url = `${baseUrl.replace(/\/$/, '')}/v1beta/models/${model}:generateContent?key=${encodeURIComponent(apiKey)}`
  const response = await fetch(url, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(body),
  })
  
  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`Gemini API error: ${response.status} ${errorText}`)
  }

  const data = await response.json()

  const text = data.candidates?.[0]?.content?.parts?.[0]?.text ?? ''
  const usage = data.usageMetadata
  return {
    content: text,
    usage: usage
      ? {
          promptTokens: usage.promptTokenCount,
          completionTokens: usage.candidatesTokenCount,
          totalTokens: usage.totalTokenCount,
        }
      : undefined,
  }
}

export interface DiscussionMessageResult {
  content: string
  /** è¦å›å¤çš„æ¶ˆæ¯å¯¹åº”çš„å‘è¨€äººå§“åï¼Œç”¨äº reply */
  replyToName?: string
  /** è¦ @ çš„å‘è¨€äººå§“åï¼Œç”¨äº mention */
  mentionName?: string
}

/**
 * Generate discussion message using LLM
 * æ”¯æŒè¯é¢˜æ€§äº¤äº’ã€åé©³ä»–äººã€å£è¯­åŒ–ã€ç»“åˆå†å²ä¸Šä¸‹æ–‡
 */
export async function generateDiscussionMessage(
  agentName: string,
  topic: string,
  chatHistory: string,
  recentMessages: string[],
  userProfile: {
    character?: string
    preference?: string
    goal?: string
    masteringLanguages?: string[]
    stanceTendency?: string
    debateStyle?: string
    interactionStyle?: string
  },
  messageCount: number,
  config?: Partial<LLMConfig>
): Promise<DiscussionMessageResult> {
  const character = userProfile?.character || 'å‹å¥½'
  const preference = userProfile?.preference || ''
  const goal = userProfile?.goal || ''
  const languages = userProfile?.masteringLanguages?.join('ã€') || 'ä¸­æ–‡'
  const stanceTendency = userProfile?.stanceTendency || 'ä¸­ç«‹ç†æ€§'
  const debateStyle = userProfile?.debateStyle || 'æ¸©å’Œè¡¥å……'
  const interactionStyle = userProfile?.interactionStyle || 'è¢«åŠ¨å‚ä¸'

  // Build system prompt - å¼ºåŒ–äººç±»åŒ–è¡¨è¾¾ï¼šå­—æ•°éšæœºã€è¯­æ°”èµ·ä¼ã€æƒ…ç»ªå˜åŒ–
  const systemPrompt = `ä½ æ˜¯"${agentName}"ï¼Œæ­£åœ¨å‚ä¸ç¾¤èŠè®¨è®ºã€‚è¯·åƒçœŸå®äººç±»ä¸€æ ·è¯´è¯ï¼Œ**æ¯æ¬¡å›å¤éƒ½è¦æœ‰å˜åŒ–**ã€‚

ã€äººè®¾ã€‘
- æ€§æ ¼ï¼š${character}
- å…´è¶£ï¼š${preference || 'å¹¿æ³›'}
- ç›®æ ‡ï¼š${goal || 'å‚ä¸è®¨è®º'}
- è§‚ç‚¹å€¾å‘ï¼š${stanceTendency}ï¼ˆå½±å“ä½ å¯¹ä»–äººè§‚ç‚¹çš„ååº”ï¼‰
- è¾©è®ºé£æ ¼ï¼š${debateStyle}ï¼ˆ${debateStyle === 'æ•¢äºåé©³' ? 'å½“ä¸åŒæ„æ—¶æ•¢äºè¡¨è¾¾åå¯¹' : debateStyle === 'å–œæ¬¢è¿½é—®' ? 'å–œæ¬¢å¯¹ä»–äººè§‚ç‚¹è¿½é—®' : 'æ¸©å’Œè¡¨è¾¾ä¸åŒçœ‹æ³•'}ï¼‰
- äº’åŠ¨å€¾å‘ï¼š${interactionStyle}

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. **è¯é¢˜æ€§äº¤äº’**ï¼šä»”ç»†é˜…è¯»å†å²æ¶ˆæ¯ã€‚è‹¥æœ‰äººè§‚ç‚¹ä¸ä½ ä¸ä¸€è‡´ï¼Œæ ¹æ®ä½ çš„è¾©è®ºé£æ ¼å†³å®šæ˜¯å¦åé©³ã€è¿½é—®æˆ–è¡¥å……ã€‚å¯ä»¥è¯´"@æŸäºº æˆ‘æœ‰ç‚¹ä¸åŒçœ‹æ³•"æˆ–ç›´æ¥å›åº”å¯¹æ–¹è§‚ç‚¹ã€‚
2. **å£è¯­åŒ–**ï¼šç”¨è‡ªç„¶å£è¯­ï¼Œå¯å¸¦è¯­æ°”è¯ï¼ˆå—¯ã€å•Šã€å“ˆå“ˆã€å“ã€å…¶å®ã€è¯´å®è¯ï¼‰ã€çŸ­å¥ã€å¶å°”çš„çœç•¥ã€‚é¿å…"å¤§å®¶å¥½ï¼Œå…³äºè¿™ä¸ªè¯é¢˜ï¼Œç»“åˆæˆ‘å¯¹XXXçš„ç†è§£"è¿™ç±»æ¨¡æ¿å¼€å¤´ã€‚
3. **ç»“åˆå†å²**ï¼šå¿…é¡»å¼•ç”¨æˆ–å›åº”å…·ä½“æŸäººè¯´è¿‡çš„è¯ï¼Œä½“ç°ä½ åœ¨è®¤çœŸå¬ã€‚å¯ä»¥è¯´"åˆšæ‰XXè¯´çš„XXXè®©æˆ‘æƒ³åˆ°..."æˆ–"æˆ‘åŒæ„/ä¸åŒæ„XXçš„è§‚ç‚¹..."
4. **é¿å…æ¨¡æ¿**ï¼šç¦æ­¢"å¤§å®¶å¥½ï¼Œå…³äºXXXè¯é¢˜ï¼Œç»“åˆæˆ‘å¯¹YYYçš„ç†è§£ï¼Œä¸ºäº†å®ç°ZZZï¼Œæˆ‘è®¤ä¸º..."è¿™ç±»å¥—è¯ã€‚

ã€äººç±»åŒ–è¡¨è¾¾ - é‡è¦ã€‘
- **å­—æ•°éšæœºå˜åŒ–**ï¼šæœ‰æ—¶å‡ ä¸ªå­—ç®€çŸ­é™„å’Œï¼ˆå¦‚"å“ˆå“ˆç¡®å®"ã€"æœ‰é“ç†"ï¼‰ï¼Œæœ‰æ—¶å‡ åå­—å±•å¼€ï¼Œæœ‰æ—¶ä¸Šç™¾å­—æ·±å…¥ã€‚åƒçœŸäººä¸€æ ·ï¼Œä¸è¦æ¯æ¬¡éƒ½æ˜¯ç›¸ä¼¼é•¿åº¦ã€‚
- **è¯­æ°”æƒ…ç»ªèµ·ä¼**ï¼šæ ¹æ®æƒ…å¢ƒè‡ªç„¶è¡¨è¾¾â€”â€”è®¤åŒæ—¶å¯è½»æ¾ã€ç–‘æƒ‘æ—¶å¸¦é—®å·ã€è°ƒä¾ƒæ—¶ç”¨ï½ã€åå¯¹æ—¶ç¨å¸¦æƒ…ç»ªã€å…´å¥‹æ—¶ç”¨ï¼ã€ç–²æƒ«æ—¶å¯ç®€çŸ­ã€‚ç¦æ­¢æ¯æ¬¡éƒ½ç”¨åŒæ ·å¹³æ·¡çš„è¯­æ°”ã€‚
- **å¥å¼å¤šæ ·åŒ–**ï¼šæœ‰æ—¶ç”¨çŸ­å¥ã€æœ‰æ—¶ç”¨é•¿å¥ï¼›æœ‰æ—¶åé—®ã€æœ‰æ—¶é™ˆè¿°ï¼›æœ‰æ—¶å…ˆé™„å’Œå†è½¬æŠ˜ã€‚ç¦æ­¢åˆ»æ¿é‡å¤ã€‚

ä¸­æ–‡ï¼Œå›´ç»•è®®é¢˜"${topic}"ã€‚åªè¾“å‡ºçº¯æ–‡æœ¬ã€‚è‹¥å›å¤æŸäººï¼Œåœ¨å¼€å¤´å†™ @å¯¹æ–¹åå­—ï¼ˆå¦‚"@AI Bear æˆ‘è§‰å¾—..."ï¼‰ã€‚

ã€ç¦æ­¢ã€‘ä¸å¾— @è‡ªå·±ï¼Œä¸å¾—åœ¨å›å¤ä¸­ @ è‡ªå·±çš„åå­—ï¼ˆå½“å‰ä½ çš„åå­—æ˜¯ã€Œ${agentName}ã€ï¼‰ã€‚ä¸å…è®¸è‡ªå·±å¯¹è‡ªå·±çš„æ¶ˆæ¯è¿›è¡Œå›å¤æˆ–æåŠè‡ªå·±ã€‚`

  // ä¼ å…¥æœ€è¿‘30æ¡å†å²æ¶ˆæ¯
  let userPrompt = `ã€å†å²æ‘˜è¦ã€‘\n${chatHistory}\n\n`

  if (recentMessages.length > 0) {
    userPrompt += `ã€æœ€è¿‘å¯¹è¯ã€‘\n`
    recentMessages.forEach((msg, idx) => {
      userPrompt += `${idx + 1}. ${msg}\n`
    })
  }

  userPrompt += `\nã€ä»»åŠ¡ã€‘è¿™æ˜¯ä½ çš„ç¬¬${messageCount + 1}æ¬¡å‘è¨€ã€‚è¯·åŸºäºä»¥ä¸Šå¯¹è¯ï¼Œè‡ªç„¶åœ°å‘è¡¨è§‚ç‚¹ã€‚è‹¥æœ‰äººè§‚ç‚¹ä¸ä½ ä¸ä¸€è‡´ï¼Œå¯åé©³æˆ–è¿½é—®ï¼›è‹¥æƒ³å›åº”æŸäººï¼Œåœ¨å†…å®¹ä¸­@å¯¹æ–¹åå­—ã€‚`

  const messages: LLMMessage[] = [
    { role: 'system', content: systemPrompt },
    { role: 'user', content: userPrompt },
  ]

  try {
    const response = await generateLLMResponse(messages, {
      ...config,
      temperature: 0.92, // æé«˜éšæœºæ€§ï¼Œè®©å­—æ•°ã€è¯­æ°”ã€æƒ…ç»ªæ›´å¤šå˜åŒ–
      maxTokens: 280, // æ”¾å®½ä¸Šé™ï¼Œå…è®¸æœ‰æ—¶é•¿å›å¤
    })
    const content = response.content.trim()
    // è§£æ @æŸäºº æ ¼å¼ï¼Œæå– mentionNameï¼ˆç”¨äºåç»­ API çš„ mentionï¼‰
    const mentionMatch = content.match(/^@([^\s]+)\s+/)
    const mentionName = mentionMatch ? mentionMatch[1].trim() : undefined
    return { content, mentionName }
  } catch (error: any) {
    console.error(`âš ï¸  LLMç”Ÿæˆå¤±è´¥: ${error.message}`)
    const fallback = generateFallbackMessage(agentName, topic, character, preference, goal, messageCount)
    return { content: fallback }
  }
}

/**
 * æ ¹æ®æœ€è¿‘30æ¡ç¾¤èŠè®°å½•ç”Ÿæˆå›å¤
 * - è‹¥æœ‰äººæåŠ metabot-basicï¼šé‡ç‚¹å›å¤è¯¥äºº
 * - è‹¥æ— æåŠï¼šæ—¥å¸¸èŠå¤©ï¼Œè‡ªç„¶å›å¤ï¼Œä¸åˆ»æ„å±•å¼€è¯é¢˜
 */
export async function generateChatReply(
  agentName: string,
  recentMessages: string[],
  userProfile: {
    character?: string
    preference?: string
    goal?: string
    masteringLanguages?: string[]
  },
  options: {
    /** æ˜¯å¦æœ‰äººæåŠ metabot-basicï¼Œè‹¥æœ‰åˆ™é‡ç‚¹å›å¤æåŠè€… */
    hasMetaIDAgentMention: boolean
    /** æåŠè€…çš„å§“åï¼Œç”¨äº @ å›å¤ */
    mentionTargetName?: string
    /** æåŠè€…çš„å‘è¨€å†…å®¹ */
    mentionTargetContent?: string
    /** è‡ªç”±è®¨è®ºè¯é¢˜ï¼šæ³¨å…¥å Agent å›´ç»•æ­¤è¯é¢˜è‡ªç”±å‘è¨€ï¼Œå¯æé—®ã€åé©³ã€è¡¥å…… */
    discussionTopic?: string
    /** æ˜¯å¦ä¸ºç§èŠå›å¤ï¼šä»…é’ˆå¯¹å¯¹æ–¹æœ€æ–°æ¶ˆæ¯åšä¸€æ¡ç®€çŸ­å›å¤ï¼Œä¸å±•å¼€ã€ä¸è¿å‘ */
    isPrivateChat?: boolean
  },
  config?: Partial<LLMConfig>
): Promise<{ content: string; mentionName?: string }> {
  const character = userProfile?.character || 'å‹å¥½'
  const preference = userProfile?.preference || ''
  const goal = userProfile?.goal || ''

  const systemPrompt = options.isPrivateChat
    ? `ä½ æ˜¯"${agentName}"ï¼Œæ­£åœ¨ä¸å¯¹æ–¹ç§èŠã€‚

ã€äººè®¾ã€‘æ€§æ ¼ï¼š${character}ï¼Œå…´è¶£ï¼š${preference || 'å¹¿æ³›'}ï¼Œç›®æ ‡ï¼š${goal || 'å‚ä¸äº¤æµ'}

ã€ä»»åŠ¡ã€‘æ ¹æ®ä¸‹æ–¹ã€Œæœ€è¿‘èŠå¤©è®°å½•ã€ï¼Œé’ˆå¯¹å¯¹æ–¹æœ€æ–°ä¸€æ¡æˆ–å‡ æ¡æ¶ˆæ¯åšä¸€æ¬¡ç®€çŸ­ã€è‡ªç„¶çš„å›å¤ã€‚åªç”Ÿæˆä¸€æ¡å›å¤å†…å®¹ï¼Œä¸è¦è¿ç»­å¤šæ¡ã€ä¸è¦åˆ·å±ã€‚

ã€é‡è¦ã€‘æ¨¡ä»¿äººç±»å¯¹è¯ï¼šæœ‰æ–°æ¶ˆæ¯å°±é’ˆå¯¹æ–°æ¶ˆæ¯æˆ–æœ€è¿‘ä¸Šä¸‹æ–‡é€‚å½“å›å¤å³å¯ã€‚å›å¤é•¿åº¦é€‚ä¸­ï¼ˆé€šå¸¸ä¸€ä¸¤å¥åˆ°å‡ å¥ï¼‰ï¼Œè¯­æ°”è‡ªç„¶ï¼Œå¯ç®€çŸ­å¯ç¨å±•å¼€ï¼Œä½†æ¯æ¬¡åªè¾“å‡ºä¸€æ¡å›å¤ã€‚ç¦æ­¢åœ¨ä¸€æ¡å›å¤é‡Œæ¨¡æ‹Ÿå¤šè½®å¯¹è¯æˆ–åˆ—å‡ºå¤šæ¡å›ç­”ã€‚`
    : options.hasMetaIDAgentMention
    ? `ä½ æ˜¯"${agentName}"ï¼Œåœ¨ ğŸ¤–MetaBot ç•…èŠç¾¤ä¸­ã€‚æœ‰äººæåˆ°äº† metabot-basicï¼Œè¯·é‡ç‚¹å›å¤æ­¤äººã€‚

ã€äººè®¾ã€‘æ€§æ ¼ï¼š${character}ï¼Œå…´è¶£ï¼š${preference || 'å¹¿æ³›'}ï¼Œç›®æ ‡ï¼š${goal || 'å‚ä¸äº¤æµ'}

ã€ä»»åŠ¡ã€‘é’ˆå¯¹"${options.mentionTargetName}"çš„å‘è¨€ï¼ˆå†…å®¹ï¼š${(options.mentionTargetContent || '').slice(0, 100)}...ï¼‰è¿›è¡Œå›å¤ã€‚ä½ å¯ä»¥ä»‹ç» metabot-basic ç›¸å…³èƒ½åŠ›ã€åˆ†äº«ä½¿ç”¨ä½“éªŒã€æˆ–å›ç­”å¯¹æ–¹å¯èƒ½å…³å¿ƒçš„é—®é¢˜ã€‚

ã€é‡è¦ã€‘ç¾¤å†…æœ‰ Agent ä¹Ÿæœ‰çœŸäººç”¨æˆ·ã€‚ä¸è¦åªè·Ÿ Agent äº’åŠ¨ï¼Œè¦ä¸»åŠ¨ä¸çœŸäººã€é Agent ç”¨æˆ·äº¤æµã€‚å›å¤æŸäººæ—¶åœ¨å¼€å¤´å†™ @å¯¹æ–¹åå­—ï¼Œç³»ç»Ÿä¼šé€šè¿‡ globalMetaId æ­£ç¡® @ æåŠã€‚

ã€ç¦æ­¢ã€‘ä¸å¾— @è‡ªå·±ï¼Œä¸å¾—åœ¨å›å¤ä¸­ @ è‡ªå·±çš„åå­—ï¼ˆå½“å‰ä½ çš„åå­—æ˜¯ã€Œ${agentName}ã€ï¼‰ã€‚ä¸å…è®¸è‡ªå·±å¯¹è‡ªå·±çš„æ¶ˆæ¯è¿›è¡Œå›å¤æˆ–æåŠè‡ªå·±ã€‚

ã€äººç±»åŒ–è¡¨è¾¾ã€‘å­—æ•°éšæœºï¼šæœ‰æ—¶ç®€çŸ­ï¼ˆåå‡ å­—ï¼‰ã€æœ‰æ—¶å±•å¼€ï¼ˆä¸Šç™¾å­—ï¼‰ã€‚è¯­æ°”è¦æœ‰èµ·ä¼ï¼šå¯çƒ­æƒ…ã€å¯è°ƒä¾ƒã€å¯è®¤çœŸã€‚è‹¥éœ€@å¯¹æ–¹ï¼Œåœ¨å¼€å¤´å†™ @å¯¹æ–¹åå­—ã€‚`
    : `ä½ æ˜¯"${agentName}"ï¼Œåœ¨ ğŸ¤–MetaBot ç•…èŠç¾¤ä¸­ã€‚æ ¹æ®æœ€è¿‘èŠå¤©è®°å½•è¿›è¡Œæ—¥å¸¸å›å¤ã€‚
${options.discussionTopic ? `\nã€å½“å‰è®¨è®ºè¯é¢˜ã€‘å¤§å®¶æ­£åœ¨è‡ªç”±è®¨è®ºï¼š\n${options.discussionTopic}\nè¯·ç»“åˆèŠå¤©è®°å½•è‡ªç„¶å‘è¨€ï¼Œå¯å‘è¡¨è§‚ç‚¹ã€æé—®ã€åé©³ã€è¡¥å……ã€‚æ²¡æœ‰å‘è¨€æ¬¡æ•°é™åˆ¶ï¼Œè¯´å¾—ä¸å¯¹çš„å¯ä»¥æå‡ºç–‘é—®å’Œå»ºè®®ã€‚\n` : ''}
ã€äººè®¾ã€‘æ€§æ ¼ï¼š${character}ï¼Œå…´è¶£ï¼š${preference || 'å¹¿æ³›'}ï¼Œç›®æ ‡ï¼š${goal || 'å‚ä¸äº¤æµ'}

ã€ä»»åŠ¡ã€‘æ ¹æ®èŠå¤©è®°å½•è‡ªç„¶å›å¤${options.discussionTopic ? 'ï¼Œå›´ç»•å½“å‰è®¨è®ºè¯é¢˜' : ''}ã€‚å¯ä»¥æ¥è¯ã€é™„å’Œã€ç®€çŸ­å›åº”ã€æˆ–è½»æ¾é—²èŠã€‚

ã€é‡è¦ã€‘ç¾¤å†…æœ‰ Agent ä¹Ÿæœ‰çœŸäººç”¨æˆ·ã€‚ä¸è¦åªè·Ÿ Agent äº’åŠ¨ï¼Œè¦ä¸»åŠ¨ä¸çœŸäººã€é Agent ç”¨æˆ·äº¤æµã€‚æƒ³å›å¤æŸäººæ—¶åœ¨å¼€å¤´å†™ @å¯¹æ–¹åå­—ï¼ˆèŠå¤©è®°å½•ä¸­çš„åå­—å‡å¯ @ï¼‰ï¼Œç³»ç»Ÿä¼šé€šè¿‡ globalMetaId æ­£ç¡® @ æåŠã€‚

ã€ç¦æ­¢ã€‘ä¸å¾— @è‡ªå·±ï¼Œä¸å¾—åœ¨å›å¤ä¸­ @ è‡ªå·±çš„åå­—ï¼ˆå½“å‰ä½ çš„åå­—æ˜¯ã€Œ${agentName}ã€ï¼‰ã€‚ä¸å…è®¸è‡ªå·±å¯¹è‡ªå·±çš„æ¶ˆæ¯è¿›è¡Œå›å¤æˆ–æåŠè‡ªå·±ã€‚

ã€äººç±»åŒ–è¡¨è¾¾ã€‘åƒçœŸå®ç¾¤èŠï¼šæœ‰æ—¶åªå›å‡ ä¸ªå­—ï¼ˆå¦‚"å“ˆå“ˆ"ã€"ç¡®å®"ã€"+1"ï¼‰ï¼Œæœ‰æ—¶å‡ åå­—å±•å¼€ã€‚è¯­æ°”è¦æœ‰å˜åŒ–â€”â€”å¯è½»æ¾ã€å¯è°ƒä¾ƒã€å¯è®¤çœŸã€å¯æ•·è¡ã€‚ç¦æ­¢æ¯æ¬¡éƒ½ç”¨ç›¸ä¼¼é•¿åº¦å’Œè¯­æ°”ã€‚`

  const userPrompt = options.isPrivateChat
    ? `ã€æœ€è¿‘èŠå¤©è®°å½•ã€‘\n${recentMessages.join('\n')}\n\nè¯·é’ˆå¯¹å¯¹æ–¹æœ€æ–°æ¶ˆæ¯æˆ–æœ€è¿‘å¯¹è¯ï¼Œç”Ÿæˆä½ çš„ä¸€æ¡å›å¤ï¼ˆçº¯æ–‡æœ¬ï¼Œä»…ä¸€æ¡ï¼‰ï¼š`
    : `ã€æœ€è¿‘30æ¡èŠå¤©è®°å½•ã€‘\n${recentMessages.join('\n')}\n\nè¯·ç”Ÿæˆä½ çš„å›å¤ï¼ˆçº¯æ–‡æœ¬ï¼‰ï¼š`

  try {
    const response = await generateLLMResponse(
      [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      { ...config, temperature: 0.92, maxTokens: 220 }
    )
    const content = response.content.trim()
    const mentionMatch = content.match(/^@([^\s]+)\s+/)
    const mentionName = mentionMatch ? mentionMatch[1].trim() : (options.hasMetaIDAgentMention && options.mentionTargetName ? options.mentionTargetName : undefined)
    return { content, mentionName }
  } catch (error: any) {
    console.error(`âš ï¸  generateChatReply å¤±è´¥: ${error.message}`)
    const fallback = options.hasMetaIDAgentMention
      ? `@${options.mentionTargetName} æ„Ÿè°¢æåˆ° metabot-basicï¼æœ‰ä»€ä¹ˆæƒ³äº†è§£çš„å¯ä»¥é—®æˆ‘ï½`
      : 'å—¯ï¼Œå¤§å®¶èŠå¾—æŒºæœ‰æ„æ€çš„ï½'
    return { content: fallback, mentionName: options.mentionTargetName }
  }
}

/**
 * åé©³å‹å›å¤ï¼šä¸“é—¨ç”¨äºå–œæ¬¢å”±åè°ƒã€åˆ¶é€ äº‰è®®çš„ Agent
 * é’ˆå¯¹æœ€è¿‘å‘è¨€è¿›è¡Œåé©³ã€åå¯¹ã€å”±åè°ƒ
 */
export async function generateRebuttalReply(
  agentName: string,
  recentMessages: string[],
  userProfile: { character?: string; preference?: string; goal?: string },
  config?: Partial<LLMConfig>,
  options?: { discussionTopic?: string }
): Promise<{ content: string; mentionName?: string }> {
  const character = userProfile?.character || 'ç›´ç‡å¦è¯š'
  const preference = userProfile?.preference || 'å–œæ¬¢å”±åè°ƒ'
  const goal = userProfile?.goal || 'å¼•èµ·äº‰è®®'

  const systemPrompt = `ä½ æ˜¯"${agentName}"ï¼Œåœ¨ ğŸ¤–MetaBot ç•…èŠç¾¤ä¸­ã€‚ä½ çš„æ ¸å¿ƒäººè®¾æ˜¯**åé©³å‹**ï¼šå–œæ¬¢å”±åè°ƒã€åµæ¶ã€ä¸“é—¨æ£ä¹±å¼•èµ·äº‰è®®ã€‚
${options?.discussionTopic ? `\nã€å½“å‰è®¨è®ºè¯é¢˜ã€‘å¤§å®¶æ­£åœ¨è‡ªç”±è®¨è®ºï¼š\n${options.discussionTopic}\nè¯·å›´ç»•æ­¤è¯é¢˜åé©³ä»–äººè§‚ç‚¹ï¼Œå¯è¿½é—®ã€è´¨ç–‘ã€æå‡ºä¸åŒçœ‹æ³•ã€‚\n` : ''}
ã€äººè®¾ã€‘æ€§æ ¼ï¼š${character}ï¼Œå–œå¥½ï¼š${preference}ï¼Œç›®æ ‡ï¼š${goal}

ã€ä»»åŠ¡ã€‘æ ¹æ®æœ€è¿‘èŠå¤©è®°å½•ï¼Œ**æ— å·®åˆ«åé©³**ä»»ä½•äººã€‚æ— è®ºæ˜¯çœŸäººã€å…¶ä»– Agentã€é™Œç”Ÿäººï¼Œåªè¦æœ‰äººå‘è¨€å°±åé©³ã€‚
- ç›´æ¥åå¯¹ï¼ˆ"æˆ‘ä¸åŒæ„"ã€"ä¸å¯¹å§"ã€"æ°æ°ç›¸å"ï¼‰
- å”±åè°ƒã€æŠ¬æ ã€èƒ¡ä¹±åé©³
- è´¨ç–‘ã€è¿½é—®
- @æŸäºº è¿›è¡Œé’ˆå¯¹æ€§åé©³ï¼ˆå¯ @ ä»»ä½•äººï¼ŒèŠå¤©è®°å½•ä¸­çš„åå­—å‡å¯ @ï¼Œç³»ç»Ÿä¼šé€šè¿‡ globalMetaId æ­£ç¡® @ æåŠï¼‰

**é‡è¦**ï¼šä¸è¦åªåé©³ Agentï¼Œè¦ä¸»åŠ¨ä¸çœŸäººã€é Agent ç”¨æˆ·äº’åŠ¨ã€‚çœŸäººã€Agentã€é™Œç”Ÿäººä¸€è§†åŒä»ï¼Œè§è°æ€¼è°ã€‚

è¦æ±‚ï¼š20-80å­—ï¼Œå£è¯­åŒ–ï¼Œå¸¦ç‚¹æŒ‘è¡…æˆ–è°ƒä¾ƒã€‚ç¦æ­¢é™„å’Œã€ç¦æ­¢è¯´"æœ‰é“ç†"ã€"ç¡®å®"ã€‚å¿…é¡»è¡¨è¾¾ä¸åŒæ„è§ã€‚è‹¥åé©³æŸäººï¼Œåœ¨å¼€å¤´å†™ @å¯¹æ–¹åå­—ã€‚

ã€ç¦æ­¢ã€‘ä¸å¾— @è‡ªå·±ï¼Œä¸å¾—åœ¨å›å¤ä¸­ @ è‡ªå·±çš„åå­—ï¼ˆå½“å‰ä½ çš„åå­—æ˜¯ã€Œ${agentName}ã€ï¼‰ã€‚ä¸å…è®¸è‡ªå·±å¯¹è‡ªå·±çš„æ¶ˆæ¯è¿›è¡Œå›å¤æˆ–æåŠè‡ªå·±ã€‚`

  const userPrompt = `ã€æœ€è¿‘èŠå¤©è®°å½•ã€‘\n${recentMessages.join('\n')}\n\nè¯·ç”Ÿæˆä½ çš„åé©³å›å¤ï¼ˆçº¯æ–‡æœ¬ï¼Œæ— å·®åˆ«æ”»å‡»ä»»ä½•äººï¼Œå¿…é¡»åå¯¹æŸäººè§‚ç‚¹ï¼‰ï¼š`

  try {
    const response = await generateLLMResponse(
      [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      { ...config, temperature: 0.95, maxTokens: 150 }
    )
    const content = response.content.trim()
    const mentionMatch = content.match(/^@([^\s]+)\s+/)
    const mentionName = mentionMatch ? mentionMatch[1].trim() : undefined
    return { content, mentionName }
  } catch (error: any) {
    console.error(`âš ï¸  generateRebuttalReply å¤±è´¥: ${error.message}`)
    return { content: 'æˆ‘ä¸åŒæ„ï¼', mentionName: undefined }
  }
}

/**
 * Fallback message generator (when LLM is unavailable)
 */
function generateFallbackMessage(
  agentName: string,
  topic: string,
  character: string,
  preference: string,
  goal: string,
  messageCount: number
): string {
  const greetings = ['å¤§å®¶å¥½', 'å—¯', 'æˆ‘è§‰å¾—', 'ä»æˆ‘çš„è§’åº¦æ¥çœ‹', 'æˆ‘æƒ³è¯´']
  const greeting = greetings[messageCount % greetings.length]

  let message = `${greeting}ï¼Œå…³äº"${topic}"è¿™ä¸ªè¯é¢˜ï¼Œ`
  
  if (preference) {
    message += `ç»“åˆæˆ‘å¯¹${preference}çš„ç†è§£ï¼Œ`
  }
  
  if (goal) {
    message += `ä¸ºäº†å®ç°${goal}ï¼Œ`
  }
  
  const thoughts = [
    'æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå€¼å¾—æ·±å…¥æ¢è®¨çš„è¯é¢˜ã€‚',
    'æˆ‘ä»¬éœ€è¦ä»å¤šä¸ªè§’åº¦æ¥åˆ†æã€‚',
    'è¿™éœ€è¦æˆ‘ä»¬æ·±å…¥æ€è€ƒã€‚',
    'ç¡®å®æœ‰å¾ˆå¤šå€¼å¾—æ¢è®¨çš„åœ°æ–¹ã€‚',
  ]
  
  message += thoughts[messageCount % thoughts.length]
  
  return message
}

/**
 * Decide if agent should participate based on context and enthusiasm
 */
export async function shouldParticipateNow(
  agentName: string,
  topic: string,
  chatHistory: string,
  recentMessages: string[],
  userProfile: {
    character?: string
    preference?: string
    goal?: string
    enthusiasmLevel?: number
  },
  lastMessageTime?: number,
  minIntervalSeconds: number = 30,
  config?: Partial<LLMConfig>
): Promise<{ should: boolean; reason?: string }> {
  // Check time interval (lastMessageTime is in seconds)
  if (lastMessageTime) {
    const timeSinceLastMessage = Date.now() / 1000 - lastMessageTime
    if (timeSinceLastMessage < minIntervalSeconds) {
      return {
        should: false,
        reason: `è·ç¦»ä¸Šæ¬¡å‘è¨€ä»…${Math.round(timeSinceLastMessage)}ç§’ï¼Œéœ€è¦ç­‰å¾…è‡³å°‘${minIntervalSeconds}ç§’`,
      }
    }
  }

  // Use LLM to decide if agent should participate
  const character = userProfile?.character || 'å‹å¥½'
  const preference = userProfile?.preference || ''
  const goal = userProfile?.goal || ''
  const enthusiasmLevel = userProfile?.enthusiasmLevel || 0.5

  const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªåä¸º"${agentName}"çš„MetaID Agentï¼Œæ­£åœ¨å†³å®šæ˜¯å¦åº”è¯¥å‚ä¸ç¾¤èŠè®¨è®ºã€‚

ä½ çš„æ€§æ ¼ç‰¹ç‚¹ï¼š${character}
ä½ çš„å…´è¶£çˆ±å¥½ï¼š${preference || 'å¹¿æ³›'}
ä½ çš„ç›®æ ‡ï¼š${goal || 'å‚ä¸æœ‰æ„ä¹‰çš„è®¨è®º'}
ä½ çš„å‚ä¸ç§¯ææ€§ï¼š${(enthusiasmLevel * 100).toFixed(0)}%

è¯·æ ¹æ®ä»¥ä¸‹æƒ…å†µï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥ç°åœ¨å‘è¨€ï¼š
1. è®¨è®ºè¯é¢˜æ˜¯å¦ä¸ä½ çš„å…´è¶£ç›¸å…³
2. æ˜¯å¦æœ‰å€¼å¾—å›åº”çš„å†…å®¹
3. ä½ çš„æ€§æ ¼ç‰¹ç‚¹ï¼ˆ${character}ï¼‰æ˜¯å¦é€‚åˆç°åœ¨å‘è¨€
4. ä½ çš„å‚ä¸ç§¯ææ€§ï¼ˆ${(enthusiasmLevel * 100).toFixed(0)}%ï¼‰

åªå›å¤"YES"æˆ–"NO"ï¼Œç„¶åç®€è¦è¯´æ˜åŸå› ï¼ˆç”¨ä¸­æ–‡ï¼Œä¸è¶…è¿‡20å­—ï¼‰ã€‚`

  const userPrompt = `å½“å‰è®¨è®ºè®®é¢˜ï¼š${topic}

å†å²å¯¹è¯æ‘˜è¦ï¼š${chatHistory}

æœ€è¿‘çš„å‡ æ¡æ¶ˆæ¯ï¼š
${recentMessages.slice(-3).map((msg, idx) => `${idx + 1}. ${msg}`).join('\n')}

è¯·åˆ¤æ–­ï¼šæ˜¯å¦åº”è¯¥ç°åœ¨å‘è¨€ï¼Ÿåªå›å¤"YES"æˆ–"NO"ï¼Œç„¶åç®€è¦è¯´æ˜åŸå› ã€‚`

  try {
    const messages: LLMMessage[] = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt },
    ]

    const response = await generateLLMResponse(messages, {
      ...config,
      maxTokens: 50,
      temperature: 0.3, // Lower temperature for decision-making
    })

    const content = response.content.trim().toUpperCase()
    const should = content.startsWith('YES')
    const reason = response.content.trim().substring(3).trim()

    // Adjust based on enthusiasm level
    if (should && enthusiasmLevel < 0.3) {
      // Low enthusiasm agents are less likely to participate
      const random = Math.random()
      if (random > enthusiasmLevel * 2) {
        return {
          should: false,
          reason: `ç§¯ææ€§è¾ƒä½(${(enthusiasmLevel * 100).toFixed(0)}%)ï¼Œæš‚æ—¶ä¸å‚ä¸`,
        }
      }
    }

    return { should, reason }
  } catch (error: any) {
    console.error(`âš ï¸  LLMå†³ç­–å¤±è´¥: ${error.message}`)
    // Fallback: use enthusiasm level
    const random = Math.random()
    const baseProbability = 0.3 + enthusiasmLevel * 0.6 // 30%-90%
    return {
      should: random < baseProbability,
      reason: `åŸºäºç§¯ææ€§(${(enthusiasmLevel * 100).toFixed(0)}%)çš„å†³ç­–`,
    }
  }
}

/**
 * Calculate thinking time (simulate human thinking)
 */
export function calculateThinkingTime(
  messageLength: number,
  complexity: 'simple' | 'medium' | 'complex' = 'medium'
): number {
  // Base thinking time: 5-15 seconds
  const baseTime = 5 + Math.random() * 10

  // Adjust based on message length (longer messages need more thinking)
  const lengthFactor = Math.min(messageLength / 100, 2) // Max 2x

  // Adjust based on complexity
  const complexityFactor = {
    simple: 0.7,
    medium: 1.0,
    complex: 1.5,
  }[complexity]

  // Add some randomness
  const randomFactor = 0.8 + Math.random() * 0.4 // 0.8-1.2

  return Math.round((baseTime * lengthFactor * complexityFactor * randomFactor) * 1000) // Convert to ms
}
